{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import itertools\n",
    "import sys\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# Normalization parameters for pre-trained PyTorch models\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "class Setup:\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.n_epochs= 200\n",
    "        self.dataset_name = \"ksdakf\"\n",
    "        self.batch_size = 4\n",
    "        self.lr = 0.0002\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.decay_epoch = 100\n",
    "        self.n_cpu = 8\n",
    "        self.hr_height = 256\n",
    "        self.hr_width = 256\n",
    "        self.channels = 3\n",
    "        self.sample_interval = 100\n",
    "        self.checkpoint_interval = 100\n",
    "        self.training_image_dir  = 'data/DIV2K_train_HR/DIV2K_train_HR'\n",
    "        self.model_checkpoint_dir = 'model/checkpoints/SRGAN'\n",
    "        self.results = 'data/results/SRGAN'\n",
    "\n",
    "\n",
    "opt = Setup()\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, hr_shape, lr_factor):\n",
    "        hr_height, hr_width = hr_shape\n",
    "        # Transforms for low resolution images and high resolution images\n",
    "        self.lr_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((hr_height // lr_factor, hr_height // lr_factor), Image.BICUBIC),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    "        )\n",
    "        self.hr_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.files = sorted(glob.glob(root + \"/*.png\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        img_lr = self.lr_transform(img)\n",
    "        img_hr = self.hr_transform(img)\n",
    "\n",
    "        return {\"lr\": img_lr, \"hr\": img_hr}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in iter(dataloader):\n",
    "#     print(data['lr'].shape,data['hr'].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_features, 0.8),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_features, 0.8),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "\n",
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16,upscale_factor=2):\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "\n",
    "        # First layer\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n",
    "\n",
    "        # Residual blocks\n",
    "        res_blocks = []\n",
    "        for _ in range(n_residual_blocks):\n",
    "            res_blocks.append(ResidualBlock(64))\n",
    "        self.res_blocks = nn.Sequential(*res_blocks)\n",
    "\n",
    "        # Second conv layer post residual blocks\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8))\n",
    "\n",
    "        # Upsampling layers\n",
    "        upsampling = []\n",
    "        for out_features in range(int(upscale_factor/2)):\n",
    "            upsampling += [\n",
    "                # nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(64, 256, 3, 1, 1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.PixelShuffle(upscale_factor=2),\n",
    "                nn.PReLU(),\n",
    "            ]\n",
    "        self.upsampling = nn.Sequential(*upsampling)\n",
    "\n",
    "        # Final output layer\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1(x)\n",
    "        out = self.res_blocks(out1)\n",
    "        out2 = self.conv2(out)\n",
    "        out = torch.add(out1, out2)\n",
    "        out = self.upsampling(out)\n",
    "        out = self.conv3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        in_channels, in_height, in_width = self.input_shape\n",
    "        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
    "        self.output_shape = (1, patch_h, patch_w)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, first_block=False):\n",
    "            layers = []\n",
    "            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n",
    "            if not first_block:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = in_channels\n",
    "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "\n",
    "\n",
    "# gen = GeneratorResNet()\n",
    "# gen(torch.ones((1,3,128,128))).shape\n",
    "# d = Discriminator((3,256,256))\n",
    "# d(torch.ones(size=(1,3,256,256))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/200] [D loss: 0.753708] [G loss: 1.415051]\n",
      "[Epoch 0/200] [Batch 1/200] [D loss: 1.246859] [G loss: 1.534688]\n",
      "[Epoch 0/200] [Batch 2/200] [D loss: 1.328590] [G loss: 1.453190]\n",
      "[Epoch 0/200] [Batch 3/200] [D loss: 0.799791] [G loss: 1.547124]\n",
      "[Epoch 0/200] [Batch 4/200] [D loss: 0.625436] [G loss: 1.595451]\n",
      "[Epoch 0/200] [Batch 5/200] [D loss: 1.084684] [G loss: 1.631242]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13040/2607426890.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mloss_G\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_content\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mloss_GAN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mloss_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        vgg19_model = vgg19(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.feature_extractor(img)\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "hr_shape = (opt.hr_height, opt.hr_width)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = GeneratorResNet()\n",
    "discriminator = Discriminator(input_shape=(opt.channels, *hr_shape))\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Set feature extractor to inference mode\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_content = torch.nn.L1Loss()\n",
    "\n",
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    feature_extractor = feature_extractor.cuda()\n",
    "    criterion_GAN = criterion_GAN.cuda()\n",
    "    criterion_content = criterion_content.cuda()\n",
    "\n",
    "if opt.epoch != 0:\n",
    "    # Load pretrained models\n",
    "    generator.load_state_dict(torch.load(opt.model_checkpoint_dir + \"/generator_%d.pth\"))\n",
    "    discriminator.load_state_dict(torch.load(opt.model_checkpoint_dir + \"/discriminator_%d.pth\"))\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "\n",
    "# dataloader = DataLoader(\n",
    "#     ImageDataset(\"../../data/%s\" % opt.dataset_name, hr_shape=hr_shape),\n",
    "#     batch_size=opt.batch_size,\n",
    "#     shuffle=True,\n",
    "#     num_workers=opt.n_cpu,\n",
    "# )\n",
    "d_set = ImageDataset(opt.training_image_dir,(opt.hr_height,opt.hr_width),2)\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    d_set,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.epoch, opt.n_epochs):\n",
    "    for i, imgs in enumerate(dataloader):\n",
    "\n",
    "        # Configure model input\n",
    "        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n",
    "        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
    "        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a high resolution image from low resolution input\n",
    "        gen_hr = generator(imgs_lr)\n",
    "\n",
    "        # Adversarial loss\n",
    "        loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n",
    "\n",
    "        # Content loss\n",
    "        gen_features = feature_extractor(gen_hr)\n",
    "        real_features = feature_extractor(imgs_hr)\n",
    "        loss_content = criterion_content(gen_features, real_features.detach())\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_content + 1e-3 * loss_GAN\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss of real and fake images\n",
    "        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n",
    "        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        sys.stdout.write(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\\n\"\n",
    "            % (epoch, opt.n_epochs, i, len(dataloader), loss_D.item(), loss_G.item())\n",
    "        )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            # Save image grid with upsampled inputs and SRGAN outputs\n",
    "            imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=2)\n",
    "            gen_hr = make_grid(gen_hr, nrow=1, normalize=True)\n",
    "            imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n",
    "            img_grid = torch.cat((imgs_lr, gen_hr), -1)\n",
    "            save_image(img_grid, opt.results + \"/%d.png\" % batches_done, normalize=False)\n",
    "\n",
    "    if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\n",
    "        # Save model checkpoints\n",
    "        torch.save(generator.state_dict(), opt.model_checkpoint_dir + \"generator_%d.pth\" % epoch)\n",
    "        torch.save(discriminator.state_dict(), opt.model_checkpoint_dir + \"/discriminator_%d.pth\" % epoch)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60cd1b893aee727141929709cd125bb2805fd7362ca3e8ee05370852b8183379"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
